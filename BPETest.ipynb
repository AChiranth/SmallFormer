{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36105c9e-f56f-4050-8f35-564f6ee873a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3001d4f9-7a3a-4d05-a6ed-d5008197b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: Optional[int] = None,\n",
    "        num_merges: Optional[int] = None,\n",
    "        end_of_word: str = \"</w>\",\n",
    "        unk_token: str = \"<unk>\",\n",
    "        add_bos: bool = False,\n",
    "        add_eos: bool = False,\n",
    "        bos_token: str = \"<bos>\",\n",
    "        eos_token: str = \"<eos>\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Either specify vocab_size or num_merges.\n",
    "\n",
    "        add_bos / add_eos:\n",
    "            If True, the tokenizer will add <bos> and/or <eos> to the encoded sequence;\n",
    "            they will also be added to the vocabulary.\n",
    "        \"\"\"\n",
    "        # Required settings\n",
    "        if vocab_size is None and num_merges is None:\n",
    "            raise ValueError(\"You must provide either vocab_size or num_merges.\")\n",
    "\n",
    "        # Parameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_merges = num_merges\n",
    "        self.end_of_word = end_of_word\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # BOS/EOS settings\n",
    "        self.add_bos = add_bos\n",
    "        self.add_eos = add_eos\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "        # Learned data\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "        self.merge_ranks: Dict[Tuple[str, str], int] = {}\n",
    "        self.alphabet: set[str] = set()\n",
    "        self.token2id: Dict[str, int] = {}\n",
    "        self.id2token: Dict[int, str] = {}\n",
    "\n",
    "        self._fitted = False\n",
    "\n",
    "    # ----- Training -----\n",
    "\n",
    "    def _build_initial_vocab(self, sentences: List[str]) -> Dict[Tuple[str, ...], int]:\n",
    "        vocab: Dict[Tuple[str, ...], int] = defaultdict(int)\n",
    "\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                symbols = list(word) + [self.end_of_word]\n",
    "                vocab[tuple(symbols)] += 1\n",
    "                self.alphabet.update(symbols)\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pair_stats(vocab):\n",
    "        stats = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                stats[(word[i], word[i + 1])] += freq\n",
    "        return stats\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_pair_in_vocab(pair, vocab):\n",
    "        merged = pair[0] + pair[1]\n",
    "        new_vocab = {}\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if (\n",
    "                    i < len(word) - 1\n",
    "                    and word[i] == pair[0]\n",
    "                    and word[i + 1] == pair[1]\n",
    "                ):\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_vocab[tuple(new_word)] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def _build_final_token_vocab(self):\n",
    "        tokens = set(self.alphabet)\n",
    "        tokens.add(self.unk_token)\n",
    "\n",
    "        # Add merge symbols\n",
    "        for a, b in self.merges:\n",
    "            tokens.add(a + b)\n",
    "\n",
    "        # Add bos/eos if required\n",
    "        if self.add_bos:\n",
    "            tokens.add(self.bos_token)\n",
    "        if self.add_eos:\n",
    "            tokens.add(self.eos_token)\n",
    "\n",
    "        tokens_list = sorted(tokens)\n",
    "        self.token2id = {t: i for i, t in enumerate(tokens_list)}\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def fit(self, sentences: List[str]):\n",
    "        vocab = self._build_initial_vocab(sentences)\n",
    "\n",
    "        initial_symbols = len(self.alphabet)\n",
    "        if self.vocab_size is not None and self.num_merges is None:\n",
    "            self.num_merges = max(0, self.vocab_size - initial_symbols)\n",
    "\n",
    "        # Learn merges\n",
    "        for _ in range(self.num_merges):\n",
    "            stats = self._get_pair_stats(vocab)\n",
    "            if not stats:\n",
    "                break\n",
    "            best = max(stats, key=stats.get)\n",
    "            self.merges.append(best)\n",
    "            vocab = self._merge_pair_in_vocab(best, vocab)\n",
    "\n",
    "        self.merge_ranks = {pair: i for i, pair in enumerate(self.merges)}\n",
    "        self._build_final_token_vocab()\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    # ----- Encoding -----\n",
    "\n",
    "    def _encode_word(self, word: str) -> List[str]:\n",
    "        symbols = list(word) + [self.end_of_word]\n",
    "\n",
    "        while True:\n",
    "            # get all pairs\n",
    "            pairs = [(symbols[i], symbols[i + 1]) for i in range(len(symbols) - 1)]\n",
    "\n",
    "            candidate = None\n",
    "            best_rank = None\n",
    "\n",
    "            for p in pairs:\n",
    "                if p in self.merge_ranks:\n",
    "                    rank = self.merge_ranks[p]\n",
    "                    if best_rank is None or rank < best_rank:\n",
    "                        best_rank = rank\n",
    "                        candidate = p\n",
    "\n",
    "            if candidate is None:\n",
    "                break\n",
    "\n",
    "            merged = candidate[0] + candidate[1]\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                if (\n",
    "                    i < len(symbols) - 1\n",
    "                    and symbols[i] == candidate[0]\n",
    "                    and symbols[i + 1] == candidate[1]\n",
    "                ):\n",
    "                    new_symbols.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "        return symbols\n",
    "\n",
    "    def encode(self, text: str, return_ids: bool = False):\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\"Tokenizer must be fitted first.\")\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        if self.add_bos:\n",
    "            tokens.append(self.bos_token)\n",
    "\n",
    "        for word in text.strip().split():\n",
    "            tokens.extend(self._encode_word(word))\n",
    "\n",
    "        if self.add_eos:\n",
    "            tokens.append(self.eos_token)\n",
    "\n",
    "        if return_ids:\n",
    "            return [self.token2id.get(t, self.token2id[self.unk_token]) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    # ----- Decoding -----\n",
    "\n",
    "    def decode(self, tokens: List[str] | List[int]) -> str:\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\"Tokenizer must be fitted first.\")\n",
    "\n",
    "        # Convert ids â†’ tokens if needed\n",
    "        if tokens and isinstance(tokens[0], int):\n",
    "            tokens = [self.id2token[t] for t in tokens]\n",
    "\n",
    "        # Strip BOS/EOS if present\n",
    "        filtered = []\n",
    "        for t in tokens:\n",
    "            if t == self.bos_token or t == self.eos_token:\n",
    "                continue\n",
    "            filtered.append(t)\n",
    "\n",
    "        words = []\n",
    "        current = \"\"\n",
    "\n",
    "        for tok in filtered:\n",
    "            if tok.endswith(self.end_of_word):\n",
    "                piece = tok[: -len(self.end_of_word)]\n",
    "                current += piece\n",
    "                words.append(current)\n",
    "                current = \"\"\n",
    "            else:\n",
    "                current += tok\n",
    "\n",
    "        if current:\n",
    "            words.append(current)\n",
    "\n",
    "        return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5f3323-dd8b-455b-a37b-c3f8d4ed03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'she</w>', 's', 'a', 'i', 'd</w>', 'it</w>', 'to</w>', 'her</w>', 'f', 'ri', 'en', 'd</w>', '<eos>']\n",
      "she said it to her friend\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog barked loudly in the yard\",\n",
    "    \"a small bird landed on the branch\",\n",
    "    \"the sun set behind the distant hills\",\n",
    "    \"she drank a cup of hot tea\",\n",
    "    \"the boy kicked the red ball\",\n",
    "    \"a gentle breeze moved through the trees\",\n",
    "    \"he opened the old wooden door slowly\",\n",
    "    \"the car stopped at the traffic light\",\n",
    "    \"they walked along the quiet road\",\n",
    "    \"the teacher explained the lesson clearly\",\n",
    "    \"a child laughed at the funny joke\",\n",
    "    \"the river flowed past the rocky shore\",\n",
    "    \"she wrote a letter to her friend\",\n",
    "    \"the wind shook the windows all night\",\n",
    "    \"a loud noise startled the sleeping cat\",\n",
    "    \"they watched a movie together last night\",\n",
    "    \"the airplane flew above the clouds\",\n",
    "    \"she baked fresh bread in the kitchen\",\n",
    "    \"the athlete ran faster than before\",\n",
    "    \"a musician played a soft melody\",\n",
    "    \"the stars shone brightly in the sky\",\n",
    "    \"the train arrived at the station\",\n",
    "    \"he found a wallet on the ground\",\n",
    "    \"the storm damaged several buildings downtown\",\n",
    "    \"a painter created a beautiful portrait\",\n",
    "    \"the chef prepared a spicy dish\",\n",
    "    \"the girl solved the puzzle quickly\",\n",
    "    \"a scientist studied the ocean currents\",\n",
    "    \"the baby cried until it fell asleep\",\n",
    "    \"the students worked quietly in the library\",\n",
    "    \"she washed her hands before dinner\",\n",
    "    \"the phone rang during the meeting\",\n",
    "    \"the clock ticked softly on the wall\",\n",
    "    \"a rabbit hopped across the field\",\n",
    "    \"the swimmer dove into the cold water\",\n",
    "    \"he tied his shoes before running\",\n",
    "    \"the clouds gathered before the rain\",\n",
    "    \"a photographer captured the perfect moment\",\n",
    "    \"the candle melted slowly on the table\",\n",
    "    \"the farmer harvested the corn crop\",\n",
    "    \"the hiker climbed the steep mountain\",\n",
    "    \"a fox darted between the trees\",\n",
    "    \"the artist sketched the city skyline\",\n",
    "    \"she whispered a secret to her sister\",\n",
    "    \"the child built a tower of blocks\",\n",
    "    \"a car engine roared down the street\",\n",
    "    \"the librarian organized the new books\",\n",
    "    \"the doctor examined the patient carefully\",\n",
    "    \"the dancer twirled gracefully on stage\",\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer = BPETokenizer(num_merges=100, add_bos=True, add_eos=True)\n",
    "tokenizer.fit(corpus)\n",
    "\n",
    "encoded = tokenizer.encode(\"she said it to her friend\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a83478-5f2c-4b38-897a-ac5a811cc088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
