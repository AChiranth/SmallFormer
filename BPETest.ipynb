{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36105c9e-f56f-4050-8f35-564f6ee873a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3001d4f9-7a3a-4d05-a6ed-d5008197b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rewritten BPETokenizer\n",
    "----------------------\n",
    "A faster, cleaner, more memory-efficient byte pair encoding tokenizer.\n",
    "Matches your original interface but is significantly more optimized.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    A Byte-Pair Encoding tokenizer (like GPT-2's BPE).\n",
    "\n",
    "    Changes & improvements explained inline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: Optional[int] = None,\n",
    "        num_merges: Optional[int] = None,\n",
    "        end_of_word=\"</w>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        add_bos=False,\n",
    "        add_eos=False,\n",
    "        bos_token=\"<bos>\",\n",
    "        eos_token=\"<eos>\"\n",
    "    ):\n",
    "        # User must specify exactly 1 of vocab_size or num_merges\n",
    "        if (vocab_size is None) == (num_merges is None):\n",
    "            raise ValueError(\"Specify exactly one: vocab_size OR num_merges.\")\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_merges = num_merges\n",
    "\n",
    "        self.end_of_word = end_of_word\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        self.add_bos = add_bos\n",
    "        self.add_eos = add_eos\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "        # Learned data structures\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "        self.merge_ranks: Dict[Tuple[str, str], int] = {}\n",
    "\n",
    "        # Token ID maps\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "        self._fitted = False\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    #                  BPE TRAINING\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    def _word_to_symbols(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert a raw word → initial list of characters + end token.\n",
    "\n",
    "        Reason:\n",
    "        - Keeping symbols as *list[str]* (not tuples) reduces memory use.\n",
    "        - This avoids your original recursion-heavy tuple reconstruction.\n",
    "        \"\"\"\n",
    "        return list(word) + [self.end_of_word]\n",
    "\n",
    "    def _build_initial_vocab(self, sentences: List[str]):\n",
    "        \"\"\"\n",
    "        Build initial vocabulary = list of word-symbol sequences with counts.\n",
    "\n",
    "        Using dict[str, int], where words are stored as space-separated symbols.\n",
    "\n",
    "        Reason:\n",
    "        - Strings are cheaper than tuples.\n",
    "        - GPT-2 stores words as 't h e </w>' strings internally.\n",
    "        \"\"\"\n",
    "        vocab = defaultdict(int)\n",
    "\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                symbols = self._word_to_symbols(word)\n",
    "                key = \" \".join(symbols)\n",
    "                vocab[key] += 1\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def _get_pair_stats(self, vocab):\n",
    "        \"\"\"\n",
    "        Count frequency of all symbol pairs in the vocab.\n",
    "\n",
    "        Reason:\n",
    "        - Much faster than your tuple-based nested loops.\n",
    "        \"\"\"\n",
    "        stats = defaultdict(int)\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                stats[pair] += freq\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _merge_pair(self, pair: Tuple[str, str], vocab):\n",
    "        \"\"\"\n",
    "        Replace all occurrences of pair (A,B) → merged token \"AB\".\n",
    "\n",
    "        Reason:\n",
    "        - Uses string replace with sentinel protection for correctness.\n",
    "        - Much faster than manual list reconstruction.\n",
    "        \"\"\"\n",
    "        merged_token = pair[0] + pair[1]\n",
    "        new_vocab = {}\n",
    "\n",
    "        pattern = \" \".join(pair)\n",
    "        replacement = merged_token\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            # replace whole pair as a unit\n",
    "            new_word = word.replace(pattern, replacement)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def fit(self, sentences: List[str]):\n",
    "        \"\"\"\n",
    "        Train BPE merges.\n",
    "        \"\"\"\n",
    "        vocab = self._build_initial_vocab(sentences)\n",
    "\n",
    "        # Initial symbols count determines num_merges if vocab_size given\n",
    "        initial_tokens = set()\n",
    "        for w in vocab:\n",
    "            initial_tokens.update(w.split())\n",
    "        initial_symbol_count = len(initial_tokens)\n",
    "\n",
    "        if self.vocab_size is not None:\n",
    "            target_merges = max(self.vocab_size - initial_symbol_count, 0)\n",
    "        else:\n",
    "            target_merges = self.num_merges\n",
    "\n",
    "        # Learn merges\n",
    "        for _ in range(target_merges):\n",
    "            stats = self._get_pair_stats(vocab)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            best = max(stats, key=stats.get)\n",
    "            self.merges.append(best)\n",
    "            vocab = self._merge_pair(best, vocab)\n",
    "\n",
    "        # Assign ranks\n",
    "        self.merge_ranks = {pair: i for i, pair in enumerate(self.merges)}\n",
    "\n",
    "        # Build actual vocab tokens\n",
    "        final_tokens = initial_tokens.copy()\n",
    "        for a, b in self.merges:\n",
    "            final_tokens.add(a + b)\n",
    "\n",
    "        final_tokens.add(self.unk_token)\n",
    "        if self.add_bos: final_tokens.add(self.bos_token)\n",
    "        if self.add_eos: final_tokens.add(self.eos_token)\n",
    "\n",
    "        sorted_tokens = sorted(final_tokens)\n",
    "        self.token2id = {tok: i for i, tok in enumerate(sorted_tokens)}\n",
    "        self.id2token = {i: tok for tok, i in self.token2id.items()}\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    #                  ENCODING\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    def _encode_word(self, word: str):\n",
    "        symbols = self._word_to_symbols(word)\n",
    "\n",
    "        # Greedy merge loop\n",
    "        while True:\n",
    "            pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n",
    "\n",
    "            best_pair = None\n",
    "            best_rank = None\n",
    "\n",
    "            for p in pairs:\n",
    "                if p in self.merge_ranks:\n",
    "                    rank = self.merge_ranks[p]\n",
    "                    if best_rank is None or rank < best_rank:\n",
    "                        best_rank = rank\n",
    "                        best_pair = p\n",
    "\n",
    "            if best_pair is None:\n",
    "                break\n",
    "\n",
    "            merged = best_pair[0] + best_pair[1]\n",
    "            new_symbols = []\n",
    "\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols)-1 and symbols[i]==best_pair[0] and symbols[i+1]==best_pair[1]:\n",
    "                    new_symbols.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "\n",
    "            symbols = new_symbols\n",
    "\n",
    "        return symbols\n",
    "\n",
    "    def encode(self, text: str, return_ids=False):\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\"Tokenizer must be fitted before encoding.\")\n",
    "\n",
    "        tokens = []\n",
    "        if self.add_bos:\n",
    "            tokens.append(self.bos_token)\n",
    "\n",
    "        for word in text.split():\n",
    "            tokens.extend(self._encode_word(word))\n",
    "\n",
    "        if self.add_eos:\n",
    "            tokens.append(self.eos_token)\n",
    "\n",
    "        if return_ids:\n",
    "            return [self.token2id.get(t, self.token2id[self.unk_token]) for t in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    #                  DECODING\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\"Tokenizer must be fitted first.\")\n",
    "\n",
    "        if tokens and isinstance(tokens[0], int):\n",
    "            tokens = [self.id2token[t] for t in tokens]\n",
    "\n",
    "        words = []\n",
    "        current = \"\"\n",
    "\n",
    "        for tok in tokens:\n",
    "            if tok in (self.bos_token, self.eos_token):\n",
    "                continue\n",
    "\n",
    "            if tok.endswith(self.end_of_word):\n",
    "                current += tok[:-len(self.end_of_word)]\n",
    "                words.append(current)\n",
    "                current = \"\"\n",
    "            else:\n",
    "                current += tok\n",
    "\n",
    "        if current:\n",
    "            words.append(current)\n",
    "\n",
    "        return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f3323-dd8b-455b-a37b-c3f8d4ed03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'she</w>', 's', 'a', 'i', 'd</w>', 'it</w>', 'to</w>', 'her</w>', 'f', 'ri', 'en', 'd</w>', '<eos>']\n",
      "she said it to her friend\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog barked loudly in the yard\",\n",
    "    \"a small bird landed on the branch\",\n",
    "    \"the sun set behind the distant hills\",\n",
    "    \"she drank a cup of hot tea\",\n",
    "    \"the boy kicked the red ball\",\n",
    "    \"a gentle breeze moved through the trees\",\n",
    "    \"he opened the old wooden door slowly\",\n",
    "    \"the car stopped at the traffic light\",\n",
    "    \"they walked along the quiet road\",\n",
    "    \"the teacher explained the lesson clearly\",\n",
    "    \"a child laughed at the funny joke\",\n",
    "    \"the river flowed past the rocky shore\",\n",
    "    \"she wrote a letter to her friend\",\n",
    "    \"the wind shook the windows all night\",\n",
    "    \"a loud noise startled the sleeping cat\",\n",
    "    \"they watched a movie together last night\",\n",
    "    \"the airplane flew above the clouds\",\n",
    "    \"she baked fresh bread in the kitchen\",\n",
    "    \"the athlete ran faster than before\",\n",
    "    \"a musician played a soft melody\",\n",
    "    \"the stars shone brightly in the sky\",\n",
    "    \"the train arrived at the station\",\n",
    "    \"he found a wallet on the ground\",\n",
    "    \"the storm damaged several buildings downtown\",\n",
    "    \"a painter created a beautiful portrait\",\n",
    "    \"the chef prepared a spicy dish\",\n",
    "    \"the girl solved the puzzle quickly\",\n",
    "    \"a scientist studied the ocean currents\",\n",
    "    \"the baby cried until it fell asleep\",\n",
    "    \"the students worked quietly in the library\",\n",
    "    \"she washed her hands before dinner\",\n",
    "    \"the phone rang during the meeting\",\n",
    "    \"the clock ticked softly on the wall\",\n",
    "    \"a rabbit hopped across the field\",\n",
    "    \"the swimmer dove into the cold water\",\n",
    "    \"he tied his shoes before running\",\n",
    "    \"the clouds gathered before the rain\",\n",
    "    \"a photographer captured the perfect moment\",\n",
    "    \"the candle melted slowly on the table\",\n",
    "    \"the farmer harvested the corn crop\",\n",
    "    \"the hiker climbed the steep mountain\",\n",
    "    \"a fox darted between the trees\",\n",
    "    \"the artist sketched the city skyline\",\n",
    "    \"she whispered a secret to her sister\",\n",
    "    \"the child built a tower of blocks\",\n",
    "    \"a car engine roared down the street\",\n",
    "    \"the librarian organized the new books\",\n",
    "    \"the doctor examined the patient carefully\",\n",
    "    \"the dancer twirled gracefully on stage\",\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer = BPETokenizer(num_merges=100, add_bos=True, add_eos=True)\n",
    "tokenizer.fit(corpus)\n",
    "\n",
    "encoded = tokenizer.encode(\"she said it to her friend\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a83478-5f2c-4b38-897a-ac5a811cc088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
